# image-caption-accessibility
AI-powered image captioning tool for visually impaired users. 

### Core Features:

Detailed Image Descriptions - Not just "a dog on grass" but "a golden retriever playing with a red ball on green grass in a park"
Text-to-Speech Integration - Automatically read captions aloud so users don't need to read screen text
Scene Understanding - Describe the overall context (indoor/outdoor, time of day, activities happening)
Object Detection + Counting - "There are 3 people in this image, 2 chairs, and 1 table"
Text Extraction (OCR) - Read any text visible in images (signs, labels, documents)
Color Information - Describe dominant colors since that's often useful context

### User-Friendly Design for Accessibility:

- Large buttons and high contrast UI
- Keyboard navigation support
- Voice commands to trigger photo capture (if using webcam)
- Multiple input methods: upload, webcam, or paste URL

### Tech Stack:

- Image Captioning: Pre-trained models like BLIP, BLIP-2, or GIT (faster and good quality)
- Object Detection: YOLO or DETR for detailed object info
- OCR: EasyOCR or Tesseract for text extraction
- Text-to-Speech: gTTS (Google Text-to-Speech) or pyttsx3
- Streamlit for the interface
---

# ğŸ”Š AI Image Narrator

An accessible image description tool designed specifically for visually impaired users. This application uses state-of-the-art AI models to generate detailed image descriptions, extract text from images, and provide audio narration.

## âœ¨ Features

- **ğŸ¤– AI Image Captioning**: Generates natural language descriptions of images using BLIP model
- **ğŸ“ Multiple Detail Levels**: Choose between brief, detailed, or very detailed descriptions
- **ğŸ“„ Text Extraction (OCR)**: Automatically detects and reads text visible in images
- **ğŸ”Š Text-to-Speech**: Converts descriptions to audio for easy listening
- **â™¿ Accessibility First**: High contrast design, keyboard navigation, screen reader friendly
- **âš¡ Fast & Free**: Runs locally or deploys easily to Streamlit Cloud
- **ğŸ”’ Privacy Focused**: No data storage - all processing happens in real-time

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd image-caption-accessibility
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run the application**
```bash
streamlit run app.py
```

4. **Open in browser**
The app will automatically open at `http://localhost:8501`

## ğŸ“¦ Project Structure

```
image-caption-accessibility/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ config.py                   # Configuration settings
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_loader.py        # Model loading utilities
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ image_processor.py     # Image preprocessing
â”‚   â”œâ”€â”€ caption_generator.py   # Caption generation
â”‚   â”œâ”€â”€ text_to_speech.py      # TTS functionality
â”‚   â””â”€â”€ ocr_processor.py       # OCR text extraction
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml            # Streamlit theme config
â”œâ”€â”€ .gitignore                 # Git ignore rules
â””â”€â”€ README.md                  # This file
```

## ğŸ¯ How to Use

1. **Upload an Image**: Click "Browse files" or drag and drop an image
2. **Choose Settings**: Select detail level and enable/disable OCR in the sidebar
3. **Analyze**: Click the "Analyze Image" button
4. **Listen**: Audio description plays automatically (if enabled) or click play
5. **Download**: Save the audio description for offline use

### Supported Image Formats
- PNG
- JPG/JPEG
- WEBP
- BMP

## ğŸ› ï¸ Technology Stack

- **Frontend**: Streamlit
- **Image Captioning**: BLIP (Salesforce)
- **OCR**: EasyOCR
- **Text-to-Speech**: gTTS (Google Text-to-Speech)
- **Deep Learning**: PyTorch, Transformers (Hugging Face)
- **Image Processing**: Pillow, OpenCV

## ğŸŒ Deployment

### Streamlit Cloud (Recommended)

1. Push your code to GitHub
2. Go to [share.streamlit.io](https://share.streamlit.io)
3. Connect your GitHub repository
4. Deploy!

### Other Options
- **Hugging Face Spaces**: Deploy as a Gradio/Streamlit Space
- **Railway.app**: One-click deployment
- **Render.com**: Free tier available
- **Local Network**: Run on local network for personal use

## âš™ï¸ Configuration

Edit `config.py` to customize:

- **Model selection**: Change the BLIP model variant
- **Caption lengths**: Adjust detail levels
- **OCR settings**: Add more languages
- **UI text**: Customize instructions and descriptions
- **Accessibility options**: Modify colors and themes

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- [ ] Add support for video description
- [ ] Multi-language support for captions
- [ ] Object detection with bounding boxes
- [ ] Batch processing for multiple images
- [ ] Camera/webcam integration
- [ ] Save history of analyzed images
- [ ] Voice commands for hands-free operation

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- **Salesforce Research**: BLIP model
- **Hugging Face**: Transformers library
- **EasyOCR Team**: OCR capabilities
- **Streamlit**: Amazing framework for ML apps

## ğŸ“§ Contact

For questions, suggestions, or feedback, please open an issue on GitHub.

## ğŸŒŸ Show Your Support

If this tool helps you or someone you know, please consider:
- â­ Starring the repository
- ğŸ› Reporting bugs or issues
- ğŸ’¡ Suggesting new features
- ğŸ”„ Sharing with others who might benefit

---

**Made with â¤ï¸ for accessibility and inclusion**